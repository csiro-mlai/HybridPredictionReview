---
nocite:
- '@*'
bibliography: learning_pdes.bib
link-citations: true
title: Learning PDEs
slug: learning_pdes
---



<div id="learning-pdes" class="section level1">
<h1>Learning PDEs</h1>
<p>Various research directions in learning approximations to (S)PDEs</p>
<p>For now, this is a scratch pad to think through some connections, ideas and shortcomings of current approaches.</p>
<p>I’m writing this in markdown, but we could totally do it in LaTeX or RMarkdown if people are into that.</p>
<div id="problems-to-solve" class="section level2">
<h2>Problems to solve</h2>
<div id="computational-efficiency" class="section level3">
<h3>Computational efficiency</h3>
<p>PDE solvers are often slow, and inference is fast with a NN.</p>
<p>However we need to be careful about conditioning; If we out neural networl is not conditional upon parameters but must be retrained between parameter updates it is probably no longer fast. Unlike inference, training is not fast.</p>
</div>
<div id="learning-latent-fields" class="section level3">
<h3>Learning latent fields</h3>
<p>Learning forcings and geophysical perperties. TBD.</p>
</div>
<div id="learning-parameters" class="section level3">
<h3>Learning parameters</h3>
<p>If we are convinced our PDE is accurate, and can be bothered implementing it in a neural net, we can estimate parameters - see <a href="#pinn">PINNs</a></p>
</div>
<div id="interpolating" class="section level3">
<h3>Interpolating</h3>
<p>An obvious way to start it to learn a PDE over a grid or mesh.
This is unsatisfactory for a variety of reasons (large GPU memory requirement, scale-dependent.)
Can we move from grid coordinates to continuous space/time?</p>
</div>
<div id="generalisationconditionalisation" class="section level3">
<h3>Generalisation/conditionalisation</h3>
<p>We would like to learn solutions that</p>
</div>
</div>
<div id="tools" class="section level2">
<h2>Tools</h2>
<div id="dropout" class="section level3">
<h3>Dropout</h3>
<p>Dropout gives me a kinda-sorta posterior predictive density for a given NN model.
How does this propagate through timesteps in a forward-PDE model?</p>
<p><span class="citation">Foong et al. (<a href="#ref-FoongPathologies2019" role="doc-biblioref">2019</a>)</span>; <span class="citation">Gal, Hron, and Kendall (<a href="#ref-GalConcrete2017" role="doc-biblioref">2017</a>)</span>; <span class="citation">Gal and Ghahramani (<a href="#ref-GalDropout2015" role="doc-biblioref">2015</a>)</span></p>
</div>
<div id="chaos-expansion" class="section level3">
<h3>Chaos expansion</h3>
<p>Learning a basis expansion for an approximate SDE</p>
</div>
<div id="pinn" class="section level3">
<h3>PINN</h3>
<p>Physics-informed neural nets, an ”implicit” representation method, give us rapid neural networks.
See <a href="https://danmackinlay.name/notebook/ml_pde.html#learning-a-pde">PINNs</a>.</p>
</div>
<div id="generic-operator-learning" class="section level3">
<h3>Generic operator learning</h3>
<p>See, e.g. <a href="https://danmackinlay.name/notebook/ml_pde.html#fourier-neural-operator">Fourier neural operators</a>.</p>
<p>These seem favourable in that they learn rapid approximations to PDEs. However, it is unclear how to conduct an error analysis upon them or do general inference.</p>
</div>
</div>
<div id="parameters" class="section level2">
<h2>Parameters</h2>
<p>Material parameters.</p>
<p>Forcings.</p>
<p>Responses.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<div id="refs">
<div id="ref-AltosaarHierarchical2019">
<p>Altosaar, Jaan, Rajesh Ranganath, and Kyle Cranmer. 2019. “Hierarchical Variational Models for Statistical Physics.” In <em>NeurIPS</em>, 5.</p>
</div>
<div id="ref-AsherReview2015">
<p>Asher, M. J., B. F. W. Croke, A. J. Jakeman, and L. J. M. Peeters. 2015. “A Review of Surrogate Models and Their Application to Groundwater Modeling.” <em>Water Resources Research</em> 51 (8): 5957–73. <a href="https://doi.org/10.1002/2015WR016967">https://doi.org/10.1002/2015WR016967</a>.</p>
</div>
<div id="ref-AtkinsonDatadriven2019">
<p>Atkinson, Steven, Waad Subber, and Liping Wang. 2019. “Data-Driven Discovery of Free-Form Governing Differential Equations.” In <em>NeurIPS</em>, 7.</p>
</div>
<div id="ref-AyedLearning2019">
<p>Ayed, Ibrahim, and Emmanuel de Bézenac. 2019. “Learning Dynamical Systems from Partial Observations.” In <em>Advances in Neural Information Processing Systems</em>, 12.</p>
</div>
<div id="ref-BhattacharyaModel2020">
<p>Bhattacharya, Kaushik, Bamdad Hosseini, Nikola B. Kovachki, and Andrew M. Stuart. 2020. “Model Reduction and Neural Networks for Parametric PDEs.” <em>arXiv:2005.03180 [Cs, Math, Stat]</em>, May. <a href="http://arxiv.org/abs/2005.03180">http://arxiv.org/abs/2005.03180</a>.</p>
</div>
<div id="ref-BonillaVariational2017">
<p>Bonilla, Edwin V. 2017. “Variational Learning of GP Models.”</p>
</div>
<div id="ref-BonillaGeneric2019">
<p>Bonilla, Edwin V., Karl Krauth, and Amir Dezfouli. 2019. “Generic Inference in Latent Gaussian Process Models.” <em>Journal of Machine Learning Research</em> 20 (117): 1–63. <a href="http://arxiv.org/abs/1609.00577">http://arxiv.org/abs/1609.00577</a>.</p>
</div>
<div id="ref-BrehmerMining2019">
<p>Brehmer, Johann, Kyle Cranmer, Siddharth Mishra-Sharma, Felix Kling, and Gilles Louppe. 2019. “Mining Gold: Improving Simulation-Based Inference with Latent Information.” In <em>NeurIPS</em>, 7.</p>
</div>
<div id="ref-CranmerLearning2019">
<p>Cranmer, Miles D, Rui Xu, Peter Battaglia, and Shirley Ho. 2019. “Learning Symbolic Physics with Graph Networks.” In <em>Machine Learning and the Physical Sciences Workshop at the 33rd Conference on Neural Information Processing Systems (NeurIPS)</em>, 6.</p>
</div>
<div id="ref-CuiEmulatorenabled2018">
<p>Cui, Tao, Luk Peeters, Dan Pagendam, Trevor Pickett, Huidong Jin, Russell S. Crosbie, Matthias Raiber, David W. Rassam, and Mat Gilfedder. 2018. “Emulator-Enabled Approximate Bayesian Computation (ABC) and Uncertainty Analysis for Computationally Expensive Groundwater Models.” <em>Journal of Hydrology</em> 564 (September): 191–207. <a href="https://doi.org/10.1016/j.jhydrol.2018.07.005">https://doi.org/10.1016/j.jhydrol.2018.07.005</a>.</p>
</div>
<div id="ref-DezfouliScalable2015">
<p>Dezfouli, Amir, and Edwin V. Bonilla. 2015. “Scalable Inference for Gaussian Process Models with Black-Box Likelihoods.” In <em>Advances in Neural Information Processing Systems 28</em>, 1414–22. NIPS’15. Cambridge, MA, USA: MIT Press.</p>
</div>
<div id="ref-FilippiRepresentation2014">
<p>Filippi, Jean-Baptiste, Vivien Mallet, and Bahaa Nader. 2014. “Representation and Evaluation of Wildfire Propagation Simulations.” <em>International Journal of Wildland Fire</em> 23 (1): 46. <a href="https://doi.org/10.1071/WF12202">https://doi.org/10.1071/WF12202</a>.</p>
</div>
<div id="ref-FoongPathologies2019">
<p>Foong, Andrew Y K, David R Burt, Yingzhen Li, and Richard E Turner. 2019. “Pathologies of Factorised Gaussian and MC Dropout Posteriors in Bayesian Neural Networks.” In <em>4th Workshop on Bayesian Deep Learning (NeurIPS 2019)</em>, 17.</p>
</div>
<div id="ref-GalDropout2015">
<p>Gal, Yarin, and Zoubin Ghahramani. 2015. “Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.” In <em>Proceedings of the 33rd International Conference on Machine Learning (ICML-16)</em>. <a href="http://arxiv.org/abs/1506.02142">http://arxiv.org/abs/1506.02142</a>.</p>
</div>
<div id="ref-GalConcrete2017">
<p>Gal, Yarin, Jiri Hron, and Alex Kendall. 2017. “Concrete Dropout.” <em>arXiv:1705.07832 [Stat]</em>, May. <a href="http://arxiv.org/abs/1705.07832">http://arxiv.org/abs/1705.07832</a>.</p>
</div>
<div id="ref-GarneloConditional2018">
<p>Garnelo, Marta, Dan Rosenbaum, Chris J. Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo J. Rezende, and S. M. Ali Eslami. 2018. “Conditional Neural Processes.” <em>arXiv:1807.01613 [Cs, Stat]</em>, July, 10. <a href="http://arxiv.org/abs/1807.01613">http://arxiv.org/abs/1807.01613</a>.</p>
</div>
<div id="ref-GladishEmulation2018">
<p>Gladish, Daniel W., Daniel E. Pagendam, Luk J. M. Peeters, Petra M. Kuhnert, and Jai Vaze. 2018. “Emulation Engines: Choice and Quantification of Uncertainty for Complex Hydrological Models.” <em>Journal of Agricultural, Biological and Environmental Statistics</em> 23 (1): 39–62. <a href="https://doi.org/10.1007/s13253-017-0308-3">https://doi.org/10.1007/s13253-017-0308-3</a>.</p>
</div>
<div id="ref-GoldsteinMachine2015">
<p>Goldstein, Evan B., and Giovanni Coco. 2015. “Machine Learning Components in Deterministic Models: Hybrid Synergy in the Age of Data.” <em>Frontiers in Environmental Science</em> 3 (April). <a href="https://doi.org/10.3389/fenvs.2015.00033">https://doi.org/10.3389/fenvs.2015.00033</a>.</p>
</div>
<div id="ref-HollLearning2019">
<p>Holl, Philipp, Nils Thuerey, and Vladlen Koltun. 2019. “Learning to Control PDEs with Differentiable Physics.” In <em>NeurIPS</em>, 5.</p>
</div>
<div id="ref-KadriOperatorvalued2016">
<p>Kadri, Hachem, Emmanuel Duflos, Philippe Preux, Stéphane Canu, Alain Rakotomamonjy, and Julien Audiffren. 2016. “Operator-Valued Kernels for Learning from Functional Response Data.” <em>The Journal of Machine Learning Research</em> 17 (1): 613–66. <a href="http://arxiv.org/abs/1510.08231">http://arxiv.org/abs/1510.08231</a>.</p>
</div>
<div id="ref-KasimTwo2020">
<p>Kasim, M. F., D. Watson-Parris, L. Deaconu, S. Oliver, P. Hatfield, D. H. Froula, G. Gregori, et al. 2020. “Up to Two Billion Times Acceleration of Scientific Simulations with Deep Neural Architecture Search.” <em>arXiv:2001.08055 [Physics, Stat]</em>, January. <a href="http://arxiv.org/abs/2001.08055">http://arxiv.org/abs/2001.08055</a>.</p>
</div>
<div id="ref-KasimMillion2019">
<p>Kasim, Muhammad, J Topp-Mugglestone, P Hatfield, D H Froula, G Gregori, M Jarvis, E Viezzer, and Sam Vinko. 2019. “A Million Times Speed up in Parameters Retrieval with Deep Learning.” In <em>NeurIPS</em>, 5.</p>
</div>
<div id="ref-KochkovLearning2020">
<p>Kochkov, Dmitrii, Alvaro Sanchez-Gonzalez, Jamie Smith, Tobias Pfaff, Peter Battaglia, and Michael P Brenner. 2020. “Learning Latent Field Dynamics of PDEs.” In, 7.</p>
</div>
<div id="ref-KrauthAutoGP2016">
<p>Krauth, Karl, Edwin V. Bonilla, Kurt Cutajar, and Maurizio Filippone. 2016. “AutoGP: Exploring the Capabilities and Limitations of Gaussian Process Models.” In <em>UAI17</em>. <a href="http://arxiv.org/abs/1610.05392">http://arxiv.org/abs/1610.05392</a>.</p>
</div>
<div id="ref-LaloyEmulation2019">
<p>Laloy, Eric, and Diederik Jacques. 2019. “Emulation of CPU-Demanding Reactive Transport Models: A Comparison of Gaussian Processes, Polynomial Chaos Expansion, and Deep Neural Networks.” <em>Computational Geosciences</em> 23 (5): 1193–1215. <a href="https://doi.org/10.1007/s10596-019-09875-y">https://doi.org/10.1007/s10596-019-09875-y</a>.</p>
</div>
<div id="ref-LewisRapid2016">
<p>Lewis, Adam, Leo Lymburner, Matthew B. J. Purss, Brendan Brooke, Ben Evans, Alex Ip, Arnold G. Dekker, et al. 2016. “Rapid, High-Resolution Detection of Environmental Change over Continental Scales from Satellite Data the Earth Observation Data Cube.” <em>International Journal of Digital Earth</em> 9 (1): 106–11. <a href="https://doi.org/10.1080/17538947.2015.1111952">https://doi.org/10.1080/17538947.2015.1111952</a>.</p>
</div>
<div id="ref-LewisAustralian2017">
<p>Lewis, Adam, Simon Oliver, Leo Lymburner, Ben Evans, Lesley Wyborn, Norman Mueller, Gregory Raevksi, et al. 2017. “The Australian Geoscience Data Cube Foundations and Lessons Learned.” <em>Remote Sensing of Environment</em> 202 (December): 276–92. <a href="https://doi.org/10.1016/j.rse.2017.03.015">https://doi.org/10.1016/j.rse.2017.03.015</a>.</p>
</div>
<div id="ref-LiFourier2020">
<p>Li, Zongyi, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. 2020. “Fourier Neural Operator for Parametric Partial Differential Equations.” <em>arXiv:2010.08895 [Cs, Math]</em>, October. <a href="http://arxiv.org/abs/2010.08895">http://arxiv.org/abs/2010.08895</a>.</p>
</div>
<div id="ref-LianNonlinear2007">
<p>Lian, Heng. 2007. “Nonlinear Functional Models for Functional Responses in Reproducing Kernel Hilbert Spaces.” <em>Canadian Journal of Statistics</em> 35 (4): 597–606. <a href="https://doi.org/10.1002/cjs.5550350410">https://doi.org/10.1002/cjs.5550350410</a>.</p>
</div>
<div id="ref-LiuStatistical2020">
<p>Liu, Xiao, Kyongmin Yeo, and Siyuan Lu. 2020. “Statistical Modeling for Spatio-Temporal Data from Stochastic Convection-Diffusion Processes.” <em>Journal of the American Statistical Association</em> 0 (0): 1–18. <a href="https://doi.org/10.1080/01621459.2020.1863223">https://doi.org/10.1080/01621459.2020.1863223</a>.</p>
</div>
<div id="ref-LuEfficient2019">
<p>Lu, Dan, and Daniel Ricciuto. 2019. “Efficient Surrogate Modeling Methods for Large-Scale Earth System Models Based on Machine-Learning Techniques.” <em>Geoscientific Model Development</em> 12 (5): 1791–1807. <a href="https://doi.org/10.5194/gmd-12-1791-2019">https://doi.org/10.5194/gmd-12-1791-2019</a>.</p>
</div>
<div id="ref-LuDeepONet2020">
<p>Lu, Lu, Pengzhan Jin, and George Em Karniadakis. 2020. “DeepONet: Learning Nonlinear Operators for Identifying Differential Equations Based on the Universal Approximation Theorem of Operators.” <em>arXiv:1910.03193 [Cs, Stat]</em>, April. <a href="http://arxiv.org/abs/1910.03193">http://arxiv.org/abs/1910.03193</a>.</p>
</div>
<div id="ref-LuDeepXDE2019">
<p>Lu, Lu, Zhiping Mao, and Xuhui Meng. 2019. “DeepXDE: A Deep Learning Library for Solving Differential Equations.” In <em>NeurIPS</em>, 6. <a href="http://arxiv.org/abs/1907.04502">http://arxiv.org/abs/1907.04502</a>.</p>
</div>
<div id="ref-MoTaylor2017">
<p>Mo, Shaoxing, Dan Lu, Xiaoqing Shi, Guannan Zhang, Ming Ye, Jianfeng Wu, and Jichun Wu. 2017. “A Taylor Expansion-Based Adaptive Design Strategy for Global Surrogate Modeling with Applications in Groundwater Modeling.” <em>Water Resources Research</em> 53 (12): 10802–23. <a href="https://doi.org/10.1002/2017WR021622">https://doi.org/10.1002/2017WR021622</a>.</p>
</div>
<div id="ref-MoDeep2019a">
<p>Mo, Shaoxing, Nicholas Zabaras, Xiaoqing Shi, and Jichun Wu. 2019. “Deep Autoregressive Neural Networks for High-Dimensional Inverse Problems in Groundwater Contaminant Source Identification.” <em>Water Resources Research</em> 55 (5): 3856–81. <a href="https://doi.org/10.1029/2018WR024638">https://doi.org/10.1029/2018WR024638</a>.</p>
</div>
<div id="ref-MoDeep2019">
<p>Mo, Shaoxing, Yinhao Zhu, Nicholas Zabaras, Xiaoqing Shi, and Jichun Wu. 2019. “Deep Convolutional Encoder-Decoder Networks for Uncertainty Quantification of Dynamic Multiphase Flow in Heterogeneous Media.” <em>Water Resources Research</em> 55 (1): 703–28. <a href="https://doi.org/10.1029/2018WR023528">https://doi.org/10.1029/2018WR023528</a>.</p>
</div>
<div id="ref-OakleyCalibration2017">
<p>Oakley, Jeremy E., and Benjamin D. Youngman. 2017. “Calibration of Stochastic Computer Simulators Using Likelihood Emulation.” <em>Technometrics</em> 59 (1): 80–92. <a href="https://doi.org/10.1080/00401706.2015.1125391">https://doi.org/10.1080/00401706.2015.1125391</a>.</p>
</div>
<div id="ref-OHaganPolynomial2013">
<p>O’Hagan, Anthony. 2013. “Polynomial Chaos: A Tutorial and Critique from a Statistician’s Perspective,” 20.</p>
</div>
<div id="ref-PaleyesEmulation2019">
<p>Paleyes, Andrei, Mark Pullin, Maren Mahsereci, Neil Lawrence, and Javier Gonzalez. 2019. “Emulation of Physical Processes with Emukit.” In <em>Advances in Neural Information Processing Systems</em>, 8.</p>
</div>
<div id="ref-ParkMachine2019">
<p>Park, Ji Hwan, Shinjae Yoo, and Balu Nadiga. 2019. “Machine Learning Climate Variability.” In <em>NeurIPS</em>, 5.</p>
</div>
<div id="ref-ParteeModel2019">
<p>Partee, Sam, Michael Ringenburg, Benjamin Robbins, and Andrew Shao. 2019. “Model Parameter Optimization: ML-Guided Trans-Resolution Tuning of Physical Models.” In <em>NeurIPS</em>. Zenodo.</p>
</div>
<div id="ref-PlumleeBayesian2017">
<p>Plumlee, Matthew. 2017. “Bayesian Calibration of Inexact Computer Models.” <em>Journal of the American Statistical Association</em> 112 (519): 1274–85. <a href="https://doi.org/10.1080/01621459.2016.1211016">https://doi.org/10.1080/01621459.2016.1211016</a>.</p>
</div>
<div id="ref-PortwoodTurbulence2019">
<p>Portwood, Gavin D, Peetak P Mitra, Mateus Dias Ribeiro, Tan Minh Nguyen, Balasubramanya T Nadiga, Juan A Saenz, Michael Chertkov, and Animesh Garg. 2019. “Turbulence Forecasting via Neural ODE.” In <em>NeurIPS</em>, 7.</p>
</div>
<div id="ref-QianLift2020">
<p>Qian, Elizabeth, Boris Kramer, Benjamin Peherstorfer, and Karen Willcox. 2020. “Lift &amp; Learn: Physics-Informed Machine Learning for Large-Scale Nonlinear Dynamical Systems.” <em>Physica D: Nonlinear Phenomena</em> 406 (May): 132401. <a href="https://doi.org/10.1016/j.physd.2020.132401">https://doi.org/10.1016/j.physd.2020.132401</a>.</p>
</div>
<div id="ref-RaissiPhysicsinformed2019">
<p>Raissi, Maziar, P. Perdikaris, and George Em Karniadakis. 2019. “Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations.” <em>Journal of Computational Physics</em> 378 (February): 686–707. <a href="https://doi.org/10.1016/j.jcp.2018.10.045">https://doi.org/10.1016/j.jcp.2018.10.045</a>.</p>
</div>
<div id="ref-RazaviReview2012">
<p>Razavi, Saman, Bryan A. Tolson, and Donald H. Burn. 2012. “Review of Surrogate Modeling in Water Resources.” <em>Water Resources Research</em> 48 (7). <a href="https://doi.org/10.1029/2011WR011527">https://doi.org/10.1029/2011WR011527</a>.</p>
</div>
<div id="ref-RezendeEquivariant2019">
<p>Rezende, Danilo J, Sébastien Racanière, Irina Higgins, and Peter Toth. 2019. “Equivariant Hamiltonian Flows.” In <em>Machine Learning and the Physical Sciences Workshop at the 33rd Conference on Neural Information Processing Systems (NeurIPS)</em>, 6.</p>
</div>
<div id="ref-RobertsHighDimensional2017">
<p>Roberts, Dale, Norman Mueller, and Alexis Mcintyre. 2017. “High-Dimensional Pixel Composites from Earth Observation Time Series.” <em>IEEE Transactions on Geoscience and Remote Sensing</em> 55 (11): 6254–64. <a href="https://doi.org/10.1109/TGRS.2017.2723896">https://doi.org/10.1109/TGRS.2017.2723896</a>.</p>
</div>
<div id="ref-RobertsExposed2019">
<p>Roberts, Dale, John Wilford, and Omar Ghattas. 2019. “Exposed Soil and Mineral Map of the Australian Continent Revealing the Land at Its Barest.” <em>Nature Communications</em> 10 (1): 5297. <a href="https://doi.org/10.1038/s41467-019-13276-1">https://doi.org/10.1038/s41467-019-13276-1</a>.</p>
</div>
<div id="ref-SahaLearning2020">
<p>Saha, Akash, and Palaniappan Balamurugan. 2020. “Learning with Operator-Valued Kernels in Reproducing Kernel Krein Spaces.” In <em>Advances in Neural Information Processing Systems</em>. Vol. 33.</p>
</div>
<div id="ref-SanchezGonzalezHamiltonian2019">
<p>Sanchez-Gonzalez, Alvaro, Victor Bapst, Peter Battaglia, and Kyle Cranmer. 2019. “Hamiltonian Graph Networks with ODE Integrators.” In <em>Machine Learning and the Physical Sciences Workshop at the 33rd Conference on Neural Information Processing Systems (NeurIPS)</em>, 11.</p>
</div>
<div id="ref-SarkarMultifidelity2019">
<p>Sarkar, Soumalya, and Michael Joly. 2019. “Multi-Fidelity Learning with Heterogeneous Domains.” In <em>NeurIPS</em>, 5.</p>
</div>
<div id="ref-SchulamReliable2017">
<p>Schulam, Peter, and Suchi Saria. 2017. “Reliable Decision Support Using Counterfactual Models.” In <em>Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, 1696–1706. NIPS’17. Red Hook, NY, USA: Curran Associates Inc. <a href="http://arxiv.org/abs/1703.10651">http://arxiv.org/abs/1703.10651</a>.</p>
</div>
<div id="ref-SiadeReduced2020">
<p>Siade, Adam J., Tao Cui, Robert N. Karelse, and Clive Hampton. 2020. “Reduced-Dimensional Gaussian Process Machine Learning for Groundwater Allocation Planning Using Swarm Theory.” <em>Water Resources Research</em> 56 (3). <a href="https://doi.org/10.1029/2019WR026061">https://doi.org/10.1029/2019WR026061</a>.</p>
</div>
<div id="ref-SigristSpate2015">
<p>Sigrist, Fabio, Hans R. Künsch, and Werner A. Stahel. 2015a. “Spate : An R Package for Spatio-Temporal Modeling with a Stochastic Advection-Diffusion Process.” <em>Journal of Statistical Software</em> 63 (14). <a href="https://doi.org/10.18637/jss.v063.i14">https://doi.org/10.18637/jss.v063.i14</a>.</p>
</div>
<div id="ref-SigristStochastic2015">
<p>———. 2015b. “Stochastic Partial Differential Equation Based Modelling of Large Space-Time Data Sets.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 77 (1): 3–33. <a href="https://doi.org/10.1111/rssb.12061">https://doi.org/10.1111/rssb.12061</a>.</p>
</div>
<div id="ref-SigristPhysics2013">
<p>Sigrist, Fabio Roman Albert. 2013. “Physics Based Dynamic Modeling of Space-Time Data.” PhD thesis, ETH Zurich. <a href="https://doi.org/10.3929/ETHZ-A-009900551">https://doi.org/10.3929/ETHZ-A-009900551</a>.</p>
</div>
<div id="ref-SitzmannImplicit2020">
<p>Sitzmann, Vincent, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. 2020. “Implicit Neural Representations with Periodic Activation Functions.” <em>arXiv:2006.09661 [Cs, Eess]</em>, June. <a href="http://arxiv.org/abs/2006.09661">http://arxiv.org/abs/2006.09661</a>.</p>
</div>
<div id="ref-TaitVariational2020">
<p>Tait, Daniel J., and Theodoros Damoulas. 2020. “Variational Autoencoding of PDE Inverse Problems.” <em>arXiv:2006.15641 [Cs, Stat]</em>, June. <a href="http://arxiv.org/abs/2006.15641">http://arxiv.org/abs/2006.15641</a>.</p>
</div>
<div id="ref-TompsonAccelerating2017">
<p>Tompson, Jonathan, Kristofer Schlachter, Pablo Sprechmann, and Ken Perlin. 2017. “Accelerating Eulerian Fluid Simulation with Convolutional Networks.” In <em>Proceedings of the 34th International Conference on Machine Learning - Volume 70</em>, 3424–33. ICML’17. Sydney, NSW, Australia: JMLR.org.</p>
</div>
<div id="ref-vanderMerweFast2007">
<p>van der Merwe, Rudolph, Todd K. Leen, Zhengdong Lu, Sergey Frolov, and Antonio M. Baptista. 2007. “Fast Neural Network Surrogates for Very High Dimensional Physics-Based Models in Computational Oceanography.” <em>Neural Networks</em>, Computational Intelligence in Earth and Environmental Sciences, 20 (4): 462–78. <a href="https://doi.org/10.1016/j.neunet.2007.04.023">https://doi.org/10.1016/j.neunet.2007.04.023</a>.</p>
</div>
<div id="ref-VernonGalaxy2014">
<p>Vernon, Ian, Michael Goldstein, and Richard Bower. 2014. “Galaxy Formation: Bayesian History Matching for the Observable Universe.” <em>Statistical Science</em> 29 (1): 81–90. <a href="https://doi.org/10.1214/12-STS412">https://doi.org/10.1214/12-STS412</a>.</p>
</div>
<div id="ref-vonRuedenCombining2020">
<p>von Rueden, Laura, Sebastian Mayer, Rafet Sifa, Christian Bauckhage, and Jochen Garcke. 2020. “Combining Machine Learning and Simulation to a Hybrid Modelling Approach: Current and Future Directions.” In <em>Advances in Intelligent Data Analysis XVIII</em>, edited by Michael R. Berthold, Ad Feelders, and Georg Krempl, 12080:548–60. Cham: Springer International Publishing. <a href="https://doi.org/10.1007/978-3-030-44584-3_43">https://doi.org/10.1007/978-3-030-44584-3_43</a>.</p>
</div>
<div id="ref-WangPixellevel2017">
<p>Wang, Dun, David W. Hogg, Daniel Foreman-Mackey, and Bernhard Schölkopf. 2017. “A Pixel-Level Model for Event Discovery in Time-Domain Imaging.” <em>arXiv:1710.02428 [Astro-Ph]</em>, October. <a href="http://arxiv.org/abs/1710.02428">http://arxiv.org/abs/1710.02428</a>.</p>
</div>
<div id="ref-WenBatchEnsemble2020">
<p>Wen, Yeming, Dustin Tran, and Jimmy Ba. 2020. “BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning.” In <em>ICLR</em>. <a href="http://arxiv.org/abs/2002.06715">http://arxiv.org/abs/2002.06715</a>.</p>
</div>
<div id="ref-WongComputational2020">
<p>Wong, Jeffrey C. 2020. “Computational Causal Inference.” <em>arXiv:2007.10979 [Cs, Stat]</em>, July. <a href="http://arxiv.org/abs/2007.10979">http://arxiv.org/abs/2007.10979</a>.</p>
</div>
<div id="ref-XuADCME2020">
<p>Xu, Kailai, and Eric Darve. 2020. “ADCME: Learning Spatially-Varying Physical Fields Using Deep Neural Networks.” In <em>arXiv:2011.11955 [Cs, Math]</em>. <a href="http://arxiv.org/abs/2011.11955">http://arxiv.org/abs/2011.11955</a>.</p>
</div>
<div id="ref-YangPhysicsInformed2020">
<p>Yang, Liu, Dongkun Zhang, and George Em Karniadakis. 2020. “Physics-Informed Generative Adversarial Networks for Stochastic Differential Equations.” <em>SIAM Journal on Scientific Computing</em> 42 (1): A292–A317. <a href="https://doi.org/10.1137/18M1225409">https://doi.org/10.1137/18M1225409</a>.</p>
</div>
<div id="ref-YuDeep2020">
<p>Yu, Xiayang, Tao Cui, J. Sreekanth, Stephane Mangeon, Rebecca Doble, Pei Xin, David Rassam, and Mat Gilfedder. 2020. “Deep Learning Emulators for Groundwater Contaminant Transport Modelling.” <em>Journal of Hydrology</em>, August, 125351. <a href="https://doi.org/10.1016/j.jhydrol.2020.125351">https://doi.org/10.1016/j.jhydrol.2020.125351</a>.</p>
</div>
<div id="ref-ZhangLearning2020">
<p>Zhang, Dongkun, Ling Guo, and George Em Karniadakis. 2020. “Learning in Modal Space: Solving Time-Dependent Stochastic PDEs Using Physics-Informed Neural Networks.” <em>SIAM Journal on Scientific Computing</em> 42 (2): A639–A665. <a href="https://doi.org/10.1137/19M1260141">https://doi.org/10.1137/19M1260141</a>.</p>
</div>
<div id="ref-ZhangQuantifying2019">
<p>Zhang, Dongkun, Lu Lu, Ling Guo, and George Em Karniadakis. 2019. “Quantifying Total Uncertainty in Physics-Informed Neural Networks for Solving Forward and Inverse Stochastic Problems.” <em>Journal of Computational Physics</em> 397 (November): 108850. <a href="https://doi.org/10.1016/j.jcp.2019.07.048">https://doi.org/10.1016/j.jcp.2019.07.048</a>.</p>
</div>
<div id="ref-ZhuPhysicsconstrained2019">
<p>Zhu, Yinhao, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis, and Paris Perdikaris. 2019. “Physics-Constrained Deep Learning for High-Dimensional Surrogate Modeling and Uncertainty Quantification Without Labeled Data.” <em>Journal of Computational Physics</em> 394 (October): 56–81. <a href="https://doi.org/10.1016/j.jcp.2019.05.024">https://doi.org/10.1016/j.jcp.2019.05.024</a>.</p>
</div>
</div>
</div>
</div>
